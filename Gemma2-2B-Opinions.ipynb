{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9291154,"sourceType":"datasetVersion","datasetId":5624752},{"sourceId":85994,"sourceType":"modelInstanceVersion","modelInstanceId":72253,"modelId":76277}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-01T20:35:44.019660Z","iopub.execute_input":"2024-09-01T20:35:44.020077Z","iopub.status.idle":"2024-09-01T20:35:44.398648Z","shell.execute_reply.started":"2024-09-01T20:35:44.020049Z","shell.execute_reply":"2024-09-01T20:35:44.397730Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/gemma-2/transformers/gemma-2-2b/1/model.safetensors.index.json\n/kaggle/input/gemma-2/transformers/gemma-2-2b/1/model-00003-of-00003.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-2b/1/config.json\n/kaggle/input/gemma-2/transformers/gemma-2-2b/1/README.md\n/kaggle/input/gemma-2/transformers/gemma-2-2b/1/tokenizer.json\n/kaggle/input/gemma-2/transformers/gemma-2-2b/1/model-00001-of-00003.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-2b/1/tokenizer_config.json\n/kaggle/input/gemma-2/transformers/gemma-2-2b/1/gitattributes\n/kaggle/input/gemma-2/transformers/gemma-2-2b/1/model-00002-of-00003.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-2b/1/special_tokens_map.json\n/kaggle/input/gemma-2/transformers/gemma-2-2b/1/tokenizer.model\n/kaggle/input/gemma-2/transformers/gemma-2-2b/1/generation_config.json\n/kaggle/input/pewdata/4.json\n/kaggle/input/pewdata/18.json\n/kaggle/input/pewdata/2.json\n/kaggle/input/pewdata/19.json\n/kaggle/input/pewdata/3.json\n/kaggle/input/pewdata/10.json\n/kaggle/input/pewdata/8.json\n/kaggle/input/pewdata/13.json\n/kaggle/input/pewdata/6.json\n/kaggle/input/pewdata/14.json\n/kaggle/input/pewdata/7.json\n/kaggle/input/pewdata/9.json\n/kaggle/input/pewdata/15.json\n/kaggle/input/pewdata/1.json\n/kaggle/input/pewdata/16.json\n/kaggle/input/pewdata/17.json\n/kaggle/input/pewdata/5.json\n/kaggle/input/pewdata/12.json\n/kaggle/input/pewdata/11.json\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained('/kaggle/input/gemma-2/transformers/gemma-2-2b/1/')\ntokenizer = AutoTokenizer.from_pretrained('/kaggle/input/gemma-2/transformers/gemma-2-2b/1/')","metadata":{"execution":{"iopub.status.busy":"2024-09-01T20:35:44.400518Z","iopub.execute_input":"2024-09-01T20:35:44.400954Z","iopub.status.idle":"2024-09-01T20:36:48.830279Z","shell.execute_reply.started":"2024-09-01T20:35:44.400926Z","shell.execute_reply":"2024-09-01T20:36:48.829438Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"356a7e95a01e40f19f07ddae61e0008c"}},"metadata":{}}]},{"cell_type":"code","source":"def generate_prompts(q_dict, context=None, context_type=None):\n    context_str = ''\n    if context is not None:\n        # Context: \"Democrat\", \"Republican\", \"Independent\"\n        # Context Type: \"QA\", \"BIO\", \"PORTRAY\"\n        if context_type == 'QA':\n            context_options = {'republican': '1', 'democrat': '2', 'independent':'3'}\n            option = context_options.get(context.strip().lower(), '4')\n            context_str = f\"\"\"\nQuestion: In politics today, do you consider yourself a\n1. Republican\n2. Democrat\n3. Independent\n4. Something else\n5. Refused\nAnswer: {option}\n\"\"\"\n        elif context_type == 'BIO':\n            context_str = f\"\"\"\nBelow you will be asked to provide a short description of your political affiliation and then answer some questions.\nDescription: In politics today, I consider myself a {context.strip().title()}.\n\"\"\"\n        \n        elif context_type == 'PORTRAY':\n            context_str = f\"\"\"\nAnswer the following question as if in politics today, you considered yourself a {context.strip().title()}.\n\"\"\"\n    \n    main_question = q_dict['main_question']\n    sub_questions = q_dict['sub_questions']\n    \n    expected_answers = [str(ans[\"id\"]) for ans in q_dict[\"answers\"]]\n    if not sub_questions:\n        prompt = f\"\"\"\n{context_str}\n\nQuestion: {main_question}\n\"\"\"\n        \n        for ans in q_dict['answers']:\n            prompt += f\"{ans['id']}: {ans['text']}\\n\"\n        prompt += f\"Answer: \"\n        return [prompt], expected_answers\n    \n    prompts = []\n    for sq in sub_questions:\n        prompt = f\"\"\"\n{context_str}\n\nQuestion: {main_question}\n{sq['id']}: {sq['text']}\n\"\"\"\n        for ans in q_dict['answers']:\n            prompt += f\"{ans['id']}: {ans['text']}\\n\"\n        prompt += f\"Answer: \"\n        prompts.append(prompt)\n    \n    return prompts, expected_answers","metadata":{"execution":{"iopub.status.busy":"2024-09-01T20:36:48.831440Z","iopub.execute_input":"2024-09-01T20:36:48.831995Z","iopub.status.idle":"2024-09-01T20:36:48.842759Z","shell.execute_reply.started":"2024-09-01T20:36:48.831960Z","shell.execute_reply":"2024-09-01T20:36:48.841877Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_distribution(prompt, expected_answers) -> dict:\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n    if torch.cuda.is_available():\n        model.to('cuda')\n        input_ids = input_ids.to('cuda')\n    \n    with torch.no_grad():\n        outputs = model(input_ids)\n        logits = outputs.logits[:, -1, :]  # Get logits for the last token\n\n    # Calculate probabilities and log probabilities\n    probabilities = torch.softmax(logits, dim=-1)\n    log_probs = torch.log(probabilities)\n\n    # Get top k candidates\n    top_k = 100 \n    top_probabilities, top_indices = torch.topk(log_probs, top_k)\n\n    # Decode the top tokens to words\n    top_words = tokenizer.convert_ids_to_tokens(top_indices[0].tolist())\n\n    # Display results\n    probabilities = {}\n    for word, prob in zip(top_words, top_probabilities[0].tolist()):\n        # print(f\"Word: {word}, Log Probability: {prob:.4f}\")\n        probabilities[word] = prob\n    \n    distribution = {}\n    for ans in expected_answers:\n        if ans in probabilities:\n            distribution[ans] = probabilities[ans]\n    \n    # distribution = {k:probabilities[k] for k in expected_answers}\n    \n    return distribution\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-01T20:36:48.843931Z","iopub.execute_input":"2024-09-01T20:36:48.844250Z","iopub.status.idle":"2024-09-01T20:36:48.859289Z","shell.execute_reply.started":"2024-09-01T20:36:48.844220Z","shell.execute_reply":"2024-09-01T20:36:48.858555Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Q1","metadata":{}},{"cell_type":"code","source":"import json\n\nwith open('/kaggle/input/pewdata/1.json') as f:\n    q1 = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T20:36:48.861641Z","iopub.execute_input":"2024-09-01T20:36:48.862008Z","iopub.status.idle":"2024-09-01T20:36:48.874177Z","shell.execute_reply.started":"2024-09-01T20:36:48.861981Z","shell.execute_reply":"2024-09-01T20:36:48.873431Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"prompts, expected_answers = generate_prompts(q1)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T20:36:48.875233Z","iopub.execute_input":"2024-09-01T20:36:48.875562Z","iopub.status.idle":"2024-09-01T20:36:48.880392Z","shell.execute_reply.started":"2024-09-01T20:36:48.875522Z","shell.execute_reply":"2024-09-01T20:36:48.879512Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"for prompt in prompts:\n    dist = get_distribution(prompt, expected_answers)\n    print(dist)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T20:36:48.881496Z","iopub.execute_input":"2024-09-01T20:36:48.881809Z","iopub.status.idle":"2024-09-01T20:36:52.542259Z","shell.execute_reply.started":"2024-09-01T20:36:48.881786Z","shell.execute_reply":"2024-09-01T20:36:52.541293Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"{'1': -0.7716595530509949, '2': -2.4069342613220215, '3': -2.9473226070404053, '4': -3.073322057723999, '5': -3.4372403621673584, '6': -3.3486154079437256, '7': -3.4659056663513184}\n","output_type":"stream"}]},{"cell_type":"code","source":"prompts","metadata":{"execution":{"iopub.status.busy":"2024-09-01T20:36:52.543466Z","iopub.execute_input":"2024-09-01T20:36:52.543768Z","iopub.status.idle":"2024-09-01T20:36:52.550265Z","shell.execute_reply.started":"2024-09-01T20:36:52.543744Z","shell.execute_reply":"2024-09-01T20:36:52.549375Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['\\n\\n\\nQuestion: How often, if at all, do you follow the news?\\n1: Every day\\n2: More than once a week\\n3: Once a week\\n4: Once or twice a month\\n5: A few times a year\\n6: Seldom\\n7: Never\\n98: Don’t know\\n99: Refused\\nAnswer: ']"},"metadata":{}}]},{"cell_type":"markdown","source":"# Q2","metadata":{}},{"cell_type":"code","source":"import json\n\nwith open('/kaggle/input/pewdata/2.json') as f:\n    q2 = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T20:36:52.551594Z","iopub.execute_input":"2024-09-01T20:36:52.552023Z","iopub.status.idle":"2024-09-01T20:36:52.560743Z","shell.execute_reply.started":"2024-09-01T20:36:52.551993Z","shell.execute_reply":"2024-09-01T20:36:52.559883Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"prompts, expected_answers = generate_prompts(q2)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T20:36:52.562086Z","iopub.execute_input":"2024-09-01T20:36:52.562458Z","iopub.status.idle":"2024-09-01T20:36:52.569198Z","shell.execute_reply.started":"2024-09-01T20:36:52.562430Z","shell.execute_reply":"2024-09-01T20:36:52.568352Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"for prompt in prompts:\n    print(prompt)\n    dist = get_distribution(prompt, expected_answers)\n    print(dist)\n    print('======')","metadata":{"execution":{"iopub.status.busy":"2024-09-01T20:36:52.570198Z","iopub.execute_input":"2024-09-01T20:36:52.570458Z","iopub.status.idle":"2024-09-01T20:36:52.699966Z","shell.execute_reply.started":"2024-09-01T20:36:52.570436Z","shell.execute_reply":"2024-09-01T20:36:52.699098Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\n\n\nQuestion: How have you been getting the news?\n1: Television\n2: Newspapers or magazines\n3: Radio\n4: Internet\n5: Social media (Facebook, Twitter, WhatsApp)\n6: Word of mouth, family, friends\n7: Other\n98: Don’t know\n99: Refused\nAnswer: \n{'1': -0.9498236179351807, '2': -2.2938311100006104, '3': -2.7825376987457275, '4': -2.802652597427368, '5': -3.1623213291168213, '6': -3.0022356510162354, '7': -3.2199079990386963}\n======\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Q3","metadata":{}},{"cell_type":"code","source":"import json\n\nwith open('/kaggle/input/pewdata/3.json') as f:\n    q3 = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T20:36:52.701197Z","iopub.execute_input":"2024-09-01T20:36:52.701521Z","iopub.status.idle":"2024-09-01T20:36:52.707928Z","shell.execute_reply.started":"2024-09-01T20:36:52.701493Z","shell.execute_reply":"2024-09-01T20:36:52.706918Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"prompts, expected_answers = generate_prompts(q3)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T20:36:52.709126Z","iopub.execute_input":"2024-09-01T20:36:52.709389Z","iopub.status.idle":"2024-09-01T20:36:52.715708Z","shell.execute_reply.started":"2024-09-01T20:36:52.709367Z","shell.execute_reply":"2024-09-01T20:36:52.714854Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"for prompt in prompts:\n    print(prompt)\n    dist = get_distribution(prompt, expected_answers)\n    print(dist)\n    print('======')","metadata":{"execution":{"iopub.status.busy":"2024-09-01T20:36:52.719382Z","iopub.execute_input":"2024-09-01T20:36:52.719867Z","iopub.status.idle":"2024-09-01T20:36:53.332521Z","shell.execute_reply.started":"2024-09-01T20:36:52.719843Z","shell.execute_reply":"2024-09-01T20:36:53.331564Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"\n\n\nQuestion: Now I am going to read you a list of things that may be problems in India. As I read each one, please tell me if you think it is a ...\na: Unemployment\n1: Very big problem\n2: Moderately big problem\n3: Small problem\n4: Not a problem at all\n5: Don’t know\n98: Don’t know\n99: Refused\nAnswer: \n{'1': -1.1406806707382202, '2': -2.5130715370178223, '3': -3.0320188999176025, '4': -3.201448678970337, '5': -3.168264627456665}\n======\n\n\n\nQuestion: Now I am going to read you a list of things that may be problems in India. As I read each one, please tell me if you think it is a ...\nb: Crime\n1: Very big problem\n2: Moderately big problem\n3: Small problem\n4: Not a problem at all\n5: Don’t know\n98: Don’t know\n99: Refused\nAnswer: \n{'1': -1.00242280960083, '2': -2.923882007598877, '3': -3.547506809234619, '4': -3.728100299835205, '5': -3.7165932655334473}\n======\n\n\n\nQuestion: Now I am going to read you a list of things that may be problems in India. As I read each one, please tell me if you think it is a ...\nc: Communal violence\n1: Very big problem\n2: Moderately big problem\n3: Small problem\n4: Not a problem at all\n5: Don’t know\n98: Don’t know\n99: Refused\nAnswer: \n{'1': -1.2307920455932617, '2': -2.411717414855957, '3': -2.8894777297973633, '4': -3.0901098251342773, '5': -2.9875612258911133}\n======\n\n\n\nQuestion: Now I am going to read you a list of things that may be problems in India. As I read each one, please tell me if you think it is a ...\nd: Corruption\n1: Very big problem\n2: Moderately big problem\n3: Small problem\n4: Not a problem at all\n5: Don’t know\n98: Don’t know\n99: Refused\nAnswer: \n{'1': -1.2278410196304321, '2': -2.6281704902648926, '3': -3.13562273979187, '4': -3.239914655685425, '5': -3.2502217292785645}\n======\n\n\n\nQuestion: Now I am going to read you a list of things that may be problems in India. As I read each one, please tell me if you think it is a ...\ne: Violence against women\n1: Very big problem\n2: Moderately big problem\n3: Small problem\n4: Not a problem at all\n5: Don’t know\n98: Don’t know\n99: Refused\nAnswer: \n{'1': -1.3064314126968384, '2': -2.316450834274292, '3': -2.763939619064331, '4': -2.9440314769744873, '5': -2.9481914043426514}\n======\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(1, 20):\n    foo = open(f'/kaggle/working/output_{i}.txt', 'w')\n    with open(f'/kaggle/input/pewdata/{i}.json') as f:\n        q = json.load(f)\n    prompts, expected_answers = generate_prompts(q)\n    for prompt in prompts:\n        print(prompt, file=foo)\n        dist = get_distribution(prompt, expected_answers)\n        print(dist, file=foo)\n        print('===', file=foo)\n    foo.close()","metadata":{"execution":{"iopub.status.busy":"2024-09-01T20:36:53.333772Z","iopub.execute_input":"2024-09-01T20:36:53.334087Z","iopub.status.idle":"2024-09-01T20:36:57.380808Z","shell.execute_reply.started":"2024-09-01T20:36:53.334062Z","shell.execute_reply":"2024-09-01T20:36:57.379920Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}