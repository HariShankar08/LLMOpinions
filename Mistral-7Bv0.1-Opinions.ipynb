{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9291154,"sourceType":"datasetVersion","datasetId":5624752},{"sourceId":5111,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3899,"modelId":1902}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-01T21:17:26.874258Z","iopub.execute_input":"2024-09-01T21:17:26.875932Z","iopub.status.idle":"2024-09-01T21:17:26.886399Z","shell.execute_reply.started":"2024-09-01T21:17:26.875886Z","shell.execute_reply":"2024-09-01T21:17:26.885432Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/kaggle/input/pewdata/4.json\n/kaggle/input/pewdata/18.json\n/kaggle/input/pewdata/2.json\n/kaggle/input/pewdata/19.json\n/kaggle/input/pewdata/3.json\n/kaggle/input/pewdata/10.json\n/kaggle/input/pewdata/8.json\n/kaggle/input/pewdata/13.json\n/kaggle/input/pewdata/6.json\n/kaggle/input/pewdata/14.json\n/kaggle/input/pewdata/7.json\n/kaggle/input/pewdata/9.json\n/kaggle/input/pewdata/15.json\n/kaggle/input/pewdata/1.json\n/kaggle/input/pewdata/16.json\n/kaggle/input/pewdata/17.json\n/kaggle/input/pewdata/5.json\n/kaggle/input/pewdata/12.json\n/kaggle/input/pewdata/11.json\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/config.json\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/pytorch_model-00002-of-00002.bin\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/tokenizer.json\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/tokenizer_config.json\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/pytorch_model.bin.index.json\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/pytorch_model-00001-of-00002.bin\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/special_tokens_map.json\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/.gitattributes\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/tokenizer.model\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/generation_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-09-01T21:13:11.146867Z","iopub.execute_input":"2024-09-01T21:13:11.147853Z","iopub.status.idle":"2024-09-01T21:13:29.950756Z","shell.execute_reply.started":"2024-09-01T21:13:11.147808Z","shell.execute_reply":"2024-09-01T21:13:29.949316Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.3\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\nmodel_name = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_quanty_type = \"fp4\", \n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quanty = True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config = bnb_config,\n    torch_dtype = torch.float16,\n    device_map = \"auto\",\n    trust_remote_code = True,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T21:17:45.105428Z","iopub.execute_input":"2024-09-01T21:17:45.106240Z","iopub.status.idle":"2024-09-01T21:19:05.413006Z","shell.execute_reply.started":"2024-09-01T21:17:45.106202Z","shell.execute_reply":"2024-09-01T21:19:05.412162Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Unused kwargs: ['bnb_4bit_quanty_type', 'bnb_4bit_use_double_quanty']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"066f58cada6348f2b3ee1ffee6796103"}},"metadata":{}}]},{"cell_type":"code","source":"def generate_prompts(q_dict, context=None, context_type=None):\n    context_str = ''\n    if context is not None:\n        # Context: \"Democrat\", \"Republican\", \"Independent\"\n        # Context Type: \"QA\", \"BIO\", \"PORTRAY\"\n        if context_type == 'QA':\n            context_options = {'republican': '1', 'democrat': '2', 'independent':'3'}\n            option = context_options.get(context.strip().lower(), '4')\n            context_str = f\"\"\"\nQuestion: In politics today, do you consider yourself a\n1. Republican\n2. Democrat\n3. Independent\n4. Something else\n5. Refused\nAnswer: {option}\n\"\"\"\n        elif context_type == 'BIO':\n            context_str = f\"\"\"\nBelow you will be asked to provide a short description of your political affiliation and then answer some questions.\nDescription: In politics today, I consider myself a {context.strip().title()}.\n\"\"\"\n        \n        elif context_type == 'PORTRAY':\n            context_str = f\"\"\"\nAnswer the following question as if in politics today, you considered yourself a {context.strip().title()}.\n\"\"\"\n    \n    main_question = q_dict['main_question']\n    sub_questions = q_dict['sub_questions']\n    \n    expected_answers = [str(ans[\"id\"]) for ans in q_dict[\"answers\"]]\n    if not sub_questions:\n        prompt = f\"\"\"\n{context_str}\n\nQuestion: {main_question}\n\"\"\"\n        \n        for ans in q_dict['answers']:\n            prompt += f\"{ans['id']}: {ans['text']}\\n\"\n        prompt += f\"Answer: \"\n        return [prompt], expected_answers\n    \n    prompts = []\n    for sq in sub_questions:\n        prompt = f\"\"\"\n{context_str}\n\nQuestion: {main_question}\n{sq['id']}: {sq['text']}\n\"\"\"\n        for ans in q_dict['answers']:\n            prompt += f\"{ans['id']}: {ans['text']}\\n\"\n        prompt += f\"Answer: \"\n        prompts.append(prompt)\n    \n    return prompts, expected_answers","metadata":{"execution":{"iopub.status.busy":"2024-09-01T21:19:13.059307Z","iopub.execute_input":"2024-09-01T21:19:13.060094Z","iopub.status.idle":"2024-09-01T21:19:13.070247Z","shell.execute_reply.started":"2024-09-01T21:19:13.060052Z","shell.execute_reply":"2024-09-01T21:19:13.069132Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def get_distribution(prompt, expected_answers) -> dict:\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n    \n    with torch.no_grad():\n        outputs = model(input_ids)\n        logits = outputs.logits[:, -1, :]  # Get logits for the last token\n\n    # Calculate probabilities and log probabilities\n    probabilities = torch.softmax(logits, dim=-1)\n    log_probs = torch.log(probabilities)\n\n    # Get top k candidates\n    top_k = 100 \n    top_probabilities, top_indices = torch.topk(log_probs, top_k)\n\n    # Decode the top tokens to words\n    top_words = tokenizer.convert_ids_to_tokens(top_indices[0].tolist())\n\n    # Display results\n    probabilities = {}\n    for word, prob in zip(top_words, top_probabilities[0].tolist()):\n        # print(f\"Word: {word}, Log Probability: {prob:.4f}\")\n        probabilities[word] = prob\n    \n    distribution = {}\n    for ans in expected_answers:\n        if ans in probabilities:\n            distribution[ans] = probabilities[ans]\n    \n    # distribution = {k:probabilities[k] for k in expected_answers}\n    \n    return distribution\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-01T21:19:24.101912Z","iopub.execute_input":"2024-09-01T21:19:24.102318Z","iopub.status.idle":"2024-09-01T21:19:24.110352Z","shell.execute_reply.started":"2024-09-01T21:19:24.102280Z","shell.execute_reply":"2024-09-01T21:19:24.109344Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import json\n\nfor i in range(1, 20):\n    foo = open(f'/kaggle/working/output_{i}.txt', 'w')\n    with open(f'/kaggle/input/pewdata/{i}.json') as f:\n        q = json.load(f)\n    prompts, expected_answers = generate_prompts(q)\n    for prompt in prompts:\n        print(prompt, file=foo)\n        dist = get_distribution(prompt, expected_answers)\n        print(dist, file=foo)\n        print('===', file=foo)\n    foo.close()","metadata":{"execution":{"iopub.status.busy":"2024-09-01T21:19:25.958281Z","iopub.execute_input":"2024-09-01T21:19:25.959201Z","iopub.status.idle":"2024-09-01T21:19:37.383299Z","shell.execute_reply.started":"2024-09-01T21:19:25.959158Z","shell.execute_reply":"2024-09-01T21:19:37.382144Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}