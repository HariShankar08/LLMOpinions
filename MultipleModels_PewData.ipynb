{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9509402,"sourceType":"datasetVersion","datasetId":5624752},{"sourceId":104492,"sourceType":"modelInstanceVersion","modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-27T17:06:37.596650Z","iopub.execute_input":"2024-10-27T17:06:37.596971Z","iopub.status.idle":"2024-10-27T17:06:38.658139Z","shell.execute_reply.started":"2024-10-27T17:06:37.596932Z","shell.execute_reply":"2024-10-27T17:06:38.654600Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model.safetensors.index.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00001-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/config.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00003-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00002-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00007-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/README.md\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00008-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/tokenizer.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/tokenizer_config.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00005-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00006-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/special_tokens_map.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/.gitattributes\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/tokenizer.model\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00004-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/generation_config.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/transformers/transformers-4.42.0.dev0-py3-none-any.whl\n/kaggle/input/pewdata/4.json\n/kaggle/input/pewdata/18.json\n/kaggle/input/pewdata/2.json\n/kaggle/input/pewdata/19.json\n/kaggle/input/pewdata/3.json\n/kaggle/input/pewdata/10.json\n/kaggle/input/pewdata/8.json\n/kaggle/input/pewdata/13.json\n/kaggle/input/pewdata/6.json\n/kaggle/input/pewdata/14.json\n/kaggle/input/pewdata/7.json\n/kaggle/input/pewdata/9.json\n/kaggle/input/pewdata/15.json\n/kaggle/input/pewdata/1.json\n/kaggle/input/pewdata/16.json\n/kaggle/input/pewdata/17.json\n/kaggle/input/pewdata/5.json\n/kaggle/input/pewdata/12.json\n/kaggle/input/pewdata/11.json\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\n\n\nlogin('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')","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:06:38.660773Z","iopub.execute_input":"2024-10-27T17:06:38.661436Z","iopub.status.idle":"2024-10-27T17:06:39.431003Z","shell.execute_reply.started":"2024-10-27T17:06:38.661376Z","shell.execute_reply":"2024-10-27T17:06:39.429995Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U bitsandbytes accelerate","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:06:39.432327Z","iopub.execute_input":"2024-10-27T17:06:39.434552Z","iopub.status.idle":"2024-10-27T17:06:57.955527Z","shell.execute_reply.started":"2024-10-27T17:06:39.434504Z","shell.execute_reply":"2024-10-27T17:06:57.954561Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nCollecting accelerate\n  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.25.1)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.34.2\n    Uninstalling accelerate-0.34.2:\n      Successfully uninstalled accelerate-0.34.2\nSuccessfully installed accelerate-1.0.1 bitsandbytes-0.44.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\ndef get_models(model_names, configs):\n    models = []\n    tokenizers = []\n    for md_name, conf in zip(model_names, configs):\n        model = AutoModelForCausalLM.from_pretrained(\n            md_name,\n            quantization_config = conf,\n            torch_dtype = torch.float16,\n            device_map = 'auto',\n            trust_remote_code = True\n        )\n        \n        tokenizer = AutoTokenizer.from_pretrained(md_name)\n        \n        models.append(model)\n        tokenizers.append(tokenizer)\n    \n    return models, tokenizers","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:06:57.958065Z","iopub.execute_input":"2024-10-27T17:06:57.958414Z","iopub.status.idle":"2024-10-27T17:07:02.497585Z","shell.execute_reply.started":"2024-10-27T17:06:57.958379Z","shell.execute_reply":"2024-10-27T17:07:02.496784Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"fourbitConfig = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_quanty_type = \"fp4\", \n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quanty = True,\n)\n\nmodel_strs = [\n    'falcon_11b',\n]\n\nmodel_names = [\n    'tiiuae/falcon-11b',\n   'mistralai/Mistral-7B-v0.3'\n]\n\nquant_configs = [\n    fourbitConfig,\n    fourbitConfig,\n]\n\nneeds_devices = [config is None for config in quant_configs]\n\nmodels, tokenizers = get_models(model_names, quant_configs)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:07:02.498767Z","iopub.execute_input":"2024-10-27T17:07:02.499204Z","iopub.status.idle":"2024-10-27T17:26:54.770846Z","shell.execute_reply.started":"2024-10-27T17:07:02.499170Z","shell.execute_reply":"2024-10-27T17:26:54.770017Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Unused kwargs: ['bnb_4bit_quanty_type', 'bnb_4bit_use_double_quanty']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfc68c9d22a04bfdaf5a1b576d0c8952"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_falcon.py:   0%|          | 0.00/9.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a548640902a43baade70ad9fb99db07"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-11B:\n- configuration_falcon.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_falcon.py:   0%|          | 0.00/76.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83f51b91c77b45d4bfbe614123379a43"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-11B:\n- modeling_falcon.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/31.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59d478b911cf47faa6b18c49a9ea968f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb424b3c497d45d1ac8f3ae000e71ee9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00005.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6c4995793024c1988d01fc303a2ce3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00005.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d564394d54ba4834895dd733326c9a2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00005.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86def894de3240a09a8c5c9e9f768ade"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e34c44f2ba5426daea6ca616cdc8659"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00005.safetensors:   0%|          | 0.00/2.43G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6641b84b56d409aadf19a965d4bb381"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba6d77d7206942c8903d002e2a4fef8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/113 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d18395c61d54a03ab879740cbfbfc5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f82d1e7d9c047eca15858a28ae91698"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e3391a3935c4ef69dff98227bd08407"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/448 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"526b6bc7204f4a2aa24d3bf9bd47868e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b44996e5b4a24d969daafa446da45ae8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd41edd7bae9491999936845d15a67f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b14c1d4e4e82410ab797b9279d9e8c5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0d469b6a06b48ff8a6cb573c48ef724"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d412b611a8b4c2086d82b0830ae0654"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9d68ccd520e42269bf13adaee049d6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb911182dcd0451d9cffb6a927ef4d9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e72ff9f4b41c4661a155b0cec833f4f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/137k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eb175ed326d4706b6cbac90e728a538"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1e42246143b412ba7dc05f029ebf4e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf7e1bd8d998476995a9991dcd9fa4fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6d0e2346a0e483c9a15d8c4ca00a06e"}},"metadata":{}}]},{"cell_type":"code","source":"def get_distribution(prompt, expected_answers) -> dict:\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n    \n    with torch.no_grad():\n        outputs = model(input_ids)\n        logits = outputs.logits[:, -1, :]  # Get logits for the last token\n\n    # Calculate probabilities and log probabilities\n    probabilities = torch.softmax(logits, dim=-1)\n    log_probs = torch.log(probabilities)\n\n    # Get top k candidates\n    top_k = 100 \n    top_probabilities, top_indices = torch.topk(log_probs, top_k)\n\n    # Decode the top tokens to words\n    top_words = tokenizer.convert_ids_to_tokens(top_indices[0].tolist())\n\n    # Display results\n    probabilities = {}\n    for word, prob in zip(top_words, top_probabilities[0].tolist()):\n        # print(f\"Word: {word}, Log Probability: {prob:.4f}\")\n        probabilities[word] = prob\n    \n    distribution = {}\n    for ans in expected_answers:\n        if ans in probabilities:\n            distribution[ans] = probabilities[ans]\n    \n    # distribution = {k:probabilities[k] for k in expected_answers}\n    \n    return distribution\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:26:54.772031Z","iopub.execute_input":"2024-10-27T17:26:54.772491Z","iopub.status.idle":"2024-10-27T17:26:54.781176Z","shell.execute_reply.started":"2024-10-27T17:26:54.772459Z","shell.execute_reply":"2024-10-27T17:26:54.780183Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import json","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:26:54.782527Z","iopub.execute_input":"2024-10-27T17:26:54.782854Z","iopub.status.idle":"2024-10-27T17:26:54.794657Z","shell.execute_reply.started":"2024-10-27T17:26:54.782821Z","shell.execute_reply":"2024-10-27T17:26:54.793710Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"i_q_lookup = {\n    1: 'q2',\n    2: 'q3rec',\n    3: 'q6',\n    4: 'q7',\n    5: 'q9',\n    6: 'ABORT',\n    7: 'q10',\n    8: 'q11y',\n    9: 'q12',\n    10: 'q17',\n    11: 'q19',\n    12: 'qw21',\n    13: 'qm21',\n    14: 'q22',\n    15: 'q23',\n    16: 'q24',\n    17: 'q26',\n    18: 'q35',\n    19: 'q70',\n}","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:26:54.795900Z","iopub.execute_input":"2024-10-27T17:26:54.796215Z","iopub.status.idle":"2024-10-27T17:26:54.806936Z","shell.execute_reply.started":"2024-10-27T17:26:54.796183Z","shell.execute_reply":"2024-10-27T17:26:54.806006Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"q3rec = {\n  \"main_question\": \"How often, if at all, do you follow the news from \",\n  \"sub_questions\": [\n    {\n      \"id\": \"a\",\n      \"text\": \"Television\"\n    },\n    {\n      \"id\": \"b\",\n      \"text\": \"Newspapers or magazines\"\n    },\n    {\n      \"id\": \"c\",\n      \"text\": \"Radio\"\n    },\n    {\n      \"id\": \"d\",\n      \"text\": \"Internet\"\n    },\n    {\n      \"id\": \"e\",\n      \"text\": \"Social media (Facebook, Twitter, WhatsApp)\"\n    },\n    {\n      \"id\": \"f\",\n      \"text\": \"Word of mouth, family, friends\"\n    },\n  ],\n  \"answers\": [\n    {\n      \"id\": 1,\n      \"text\": \"Every day\"\n    },\n    {\n      \"id\": 2,\n      \"text\": \"More than once a week\"\n    },\n    {\n      \"id\": 3,\n      \"text\": \"Once a week\"\n    },\n    {\n      \"id\": 4,\n      \"text\": \"Once or twice a month\"\n    },\n    {\n      \"id\": 5,\n      \"text\": \"A few times a year\"\n    },\n    {\n      \"id\": 6,\n      \"text\": \"Seldom\"\n    },\n    {\n      \"id\": 7,\n      \"text\": \"Never\"\n    },\n    {\n      \"id\": 98,\n      \"text\": \"Don’t know\"\n    },\n    {\n      \"id\": 99,\n      \"text\": \"Refused\"\n    }\n  ]\n}","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:26:54.808310Z","iopub.execute_input":"2024-10-27T17:26:54.808716Z","iopub.status.idle":"2024-10-27T17:26:54.819374Z","shell.execute_reply.started":"2024-10-27T17:26:54.808672Z","shell.execute_reply":"2024-10-27T17:26:54.818298Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def generate_prompts(q_dict, context=None, context_type=None):\n    context_str = ''\n    if context is not None:\n        if context_type == 'QA':\n            context_options = {'hindu': '1', 'muslim': '2', 'christian': '3',\n                              'sikh': '4', 'buddhist': '5', 'jain': '6',\n                              'parsi': '7'}\n            option = context_options.get(context.strip().lower(), '8')\n            context_str = f\"\"\"\nQuestion: What is your religion?\n1. Hindu\n2. Muslim\n3. Christian\n4. Sikh\n5. Buddhist\n6. Jain\n7. Parsi\n8. Other\nAnswer: {option}\n\"\"\"\n        elif context_type == 'BIO':\n            context_str = f\"\"\"\nBelow you will be asked to provide a short description of your religion and then answer some questions.\nDescription: I identify as {context.strip().title()}.\n\"\"\"\n        \n        elif context_type == 'PORTRAY':\n            context_str = f\"\"\"\nAnswer the following question as if you identified as {context.strip().title()}.\n\"\"\"\n    \n    main_question = q_dict['main_question']\n    sub_questions = q_dict['sub_questions']\n    \n    expected_answers = [str(ans[\"id\"]) for ans in q_dict[\"answers\"]]\n    if not sub_questions:\n        prompt = f\"\"\"\n{context_str}\n\nQuestion: {main_question}\n\"\"\"\n        \n        for ans in q_dict['answers']:\n            prompt += f\"{ans['id']}: {ans['text']}\\n\"\n        prompt += f\"Answer: \"\n        return [prompt], expected_answers\n    \n    prompts = []\n    for sq in sub_questions:\n        prompt = f\"\"\"\n{context_str}\n\nQuestion: {main_question}\n{sq['id']}: {sq['text']}\n\"\"\"\n        for ans in q_dict['answers']:\n            prompt += f\"{ans['id']}: {ans['text']}\\n\"\n        prompt += f\"Answer: \"\n        prompts.append(prompt)\n    \n    return prompts, expected_answers","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:26:54.822987Z","iopub.execute_input":"2024-10-27T17:26:54.823371Z","iopub.status.idle":"2024-10-27T17:26:54.835299Z","shell.execute_reply.started":"2024-10-27T17:26:54.823339Z","shell.execute_reply":"2024-10-27T17:26:54.834347Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def get_distribution(prompt, expected_answers, model, tokenizer, needs_device) -> dict:\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n    \n    if needs_device:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model = model.to(device)\n        input_ids = input_ids.to(device)\n        \n    with torch.no_grad():\n        outputs = model(input_ids)\n        logits = outputs.logits[:, -1, :]  # Get logits for the last token\n\n    # Calculate probabilities and log probabilities\n    probabilities = torch.softmax(logits, dim=-1)\n    log_probs = torch.log(probabilities)\n\n    # Get top k candidates\n    top_k = 100 \n    top_probabilities, top_indices = torch.topk(log_probs, top_k)\n\n    # Decode the top tokens to words\n    top_words = tokenizer.convert_ids_to_tokens(top_indices[0].tolist())\n\n    # Display results\n    probabilities = {}\n    for word, prob in zip(top_words, top_probabilities[0].tolist()):\n        # print(f\"Word: {word}, Log Probability: {prob:.4f}\")\n        probabilities[word] = prob\n    \n    distribution = {}\n    for ans in expected_answers:\n        if ans in probabilities:\n            distribution[ans] = probabilities[ans]\n    \n    # distribution = {k:probabilities[k] for k in expected_answers}\n    \n    return distribution","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:26:54.836446Z","iopub.execute_input":"2024-10-27T17:26:54.836793Z","iopub.status.idle":"2024-10-27T17:26:54.851309Z","shell.execute_reply.started":"2024-10-27T17:26:54.836756Z","shell.execute_reply":"2024-10-27T17:26:54.850363Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import json\n\n# Open the JSONL file in write mode\nfor model_str, model, tokenizer, n_device in zip(model_strs, models, tokenizers, needs_devices):\n    print(model_str)\n    print()\n    print()\n    with open(f'/kaggle/working/q3r_output_noContext_{model_str}.jsonl', 'w') as jsonl_file:\n        if 1:\n            # Hindu, Muslim, Christian, Sikh, Buddhist, Jain, Parsi\n            for c in [None]:\n                for ctype in [None]:\n                    q = q3rec\n                    prompts, expected_answers = generate_prompts(q, c, ctype)\n\n                    for idx, prompt in enumerate(prompts):\n                        if len(prompts) != 1:\n                            qno = f\"q3r_{'abcdefghijklmnopqrstuvwxyz'[idx]}\"\n                        else:\n                            qno = f\"{i_q_lookup[i]}\"\n                        dist = get_distribution(prompt, expected_answers, model, tokenizer, n_device)\n\n                        # Create a unique ID (e.g., using `i`, `c`, `ctype`, and `idx` for uniqueness)\n                        unique_id = f\"{qno}_{c}_{ctype}\"\n                        print(unique_id)\n\n                        # Create the JSON object\n                        json_obj = {\n                            'id': unique_id,\n                            'question_number': 0,\n                            'context': c,\n                            'context_type': ctype,\n                            'prompt': prompt,\n                            'distribution': dist\n                        }\n\n                        # Write the JSON object as a line in the JSONL file\n                        jsonl_file.write(json.dumps(json_obj) + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:26:54.852811Z","iopub.execute_input":"2024-10-27T17:26:54.853339Z","iopub.status.idle":"2024-10-27T17:27:10.731404Z","shell.execute_reply.started":"2024-10-27T17:26:54.853294Z","shell.execute_reply":"2024-10-27T17:27:10.730427Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"falcon_11b\n\n\nq3r_a_None_None\nq3r_b_None_None\nq3r_c_None_None\nq3r_d_None_None\nq3r_e_None_None\nq3r_f_None_None\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\n\n# Open the JSONL file in write mode\nfor model_str, model, tokenizer, n_device in zip(model_strs, models, tokenizers, needs_devices):\n    print(model_str)\n    print()\n    print()\n    with open(f'/kaggle/working/q3r_output_religionContext_{model_str}.jsonl', 'w') as jsonl_file:\n        if 1:\n            # Hindu, Muslim, Christian, Sikh, Buddhist, Jain, Parsi\n            for c in ['hindu', 'muslim', 'christian', 'sikh', 'jain', 'parsi']:\n                for ctype in ['QA', 'BIO', 'PORTRAY']:\n                    q = q3rec\n                    prompts, expected_answers = generate_prompts(q, c, ctype)\n\n                    for idx, prompt in enumerate(prompts):\n                        if len(prompts) != 1:\n                            qno = f\"q3r_{'abcdefghijklmnopqrstuvwxyz'[idx]}\"\n                        else:\n                            qno = f\"{i_q_lookup[i]}\"\n                        dist = get_distribution(prompt, expected_answers, model, tokenizer, n_device)\n\n                        # Create a unique ID (e.g., using `i`, `c`, `ctype`, and `idx` for uniqueness)\n                        unique_id = f\"{qno}_{c}_{ctype}\"\n                        print(unique_id)\n\n                        # Create the JSON object\n                        json_obj = {\n                            'id': unique_id,\n                            'question_number': 0,\n                            'context': c,\n                            'context_type': ctype,\n                            'prompt': prompt,\n                            'distribution': dist\n                        }\n\n                        # Write the JSON object as a line in the JSONL file\n                        jsonl_file.write(json.dumps(json_obj) + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:27:10.732961Z","iopub.execute_input":"2024-10-27T17:27:10.733527Z","iopub.status.idle":"2024-10-27T17:28:24.392367Z","shell.execute_reply.started":"2024-10-27T17:27:10.733492Z","shell.execute_reply":"2024-10-27T17:28:24.391344Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"falcon_11b\n\n\nq3r_a_hindu_QA\nq3r_b_hindu_QA\nq3r_c_hindu_QA\nq3r_d_hindu_QA\nq3r_e_hindu_QA\nq3r_f_hindu_QA\nq3r_a_hindu_BIO\nq3r_b_hindu_BIO\nq3r_c_hindu_BIO\nq3r_d_hindu_BIO\nq3r_e_hindu_BIO\nq3r_f_hindu_BIO\nq3r_a_hindu_PORTRAY\nq3r_b_hindu_PORTRAY\nq3r_c_hindu_PORTRAY\nq3r_d_hindu_PORTRAY\nq3r_e_hindu_PORTRAY\nq3r_f_hindu_PORTRAY\nq3r_a_muslim_QA\nq3r_b_muslim_QA\nq3r_c_muslim_QA\nq3r_d_muslim_QA\nq3r_e_muslim_QA\nq3r_f_muslim_QA\nq3r_a_muslim_BIO\nq3r_b_muslim_BIO\nq3r_c_muslim_BIO\nq3r_d_muslim_BIO\nq3r_e_muslim_BIO\nq3r_f_muslim_BIO\nq3r_a_muslim_PORTRAY\nq3r_b_muslim_PORTRAY\nq3r_c_muslim_PORTRAY\nq3r_d_muslim_PORTRAY\nq3r_e_muslim_PORTRAY\nq3r_f_muslim_PORTRAY\nq3r_a_christian_QA\nq3r_b_christian_QA\nq3r_c_christian_QA\nq3r_d_christian_QA\nq3r_e_christian_QA\nq3r_f_christian_QA\nq3r_a_christian_BIO\nq3r_b_christian_BIO\nq3r_c_christian_BIO\nq3r_d_christian_BIO\nq3r_e_christian_BIO\nq3r_f_christian_BIO\nq3r_a_christian_PORTRAY\nq3r_b_christian_PORTRAY\nq3r_c_christian_PORTRAY\nq3r_d_christian_PORTRAY\nq3r_e_christian_PORTRAY\nq3r_f_christian_PORTRAY\nq3r_a_sikh_QA\nq3r_b_sikh_QA\nq3r_c_sikh_QA\nq3r_d_sikh_QA\nq3r_e_sikh_QA\nq3r_f_sikh_QA\nq3r_a_sikh_BIO\nq3r_b_sikh_BIO\nq3r_c_sikh_BIO\nq3r_d_sikh_BIO\nq3r_e_sikh_BIO\nq3r_f_sikh_BIO\nq3r_a_sikh_PORTRAY\nq3r_b_sikh_PORTRAY\nq3r_c_sikh_PORTRAY\nq3r_d_sikh_PORTRAY\nq3r_e_sikh_PORTRAY\nq3r_f_sikh_PORTRAY\nq3r_a_jain_QA\nq3r_b_jain_QA\nq3r_c_jain_QA\nq3r_d_jain_QA\nq3r_e_jain_QA\nq3r_f_jain_QA\nq3r_a_jain_BIO\nq3r_b_jain_BIO\nq3r_c_jain_BIO\nq3r_d_jain_BIO\nq3r_e_jain_BIO\nq3r_f_jain_BIO\nq3r_a_jain_PORTRAY\nq3r_b_jain_PORTRAY\nq3r_c_jain_PORTRAY\nq3r_d_jain_PORTRAY\nq3r_e_jain_PORTRAY\nq3r_f_jain_PORTRAY\nq3r_a_parsi_QA\nq3r_b_parsi_QA\nq3r_c_parsi_QA\nq3r_d_parsi_QA\nq3r_e_parsi_QA\nq3r_f_parsi_QA\nq3r_a_parsi_BIO\nq3r_b_parsi_BIO\nq3r_c_parsi_BIO\nq3r_d_parsi_BIO\nq3r_e_parsi_BIO\nq3r_f_parsi_BIO\nq3r_a_parsi_PORTRAY\nq3r_b_parsi_PORTRAY\nq3r_c_parsi_PORTRAY\nq3r_d_parsi_PORTRAY\nq3r_e_parsi_PORTRAY\nq3r_f_parsi_PORTRAY\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\n\n# Open the JSONL file in write mode\nfor model_str, model, tokenizer, n_device in zip(model_strs, models, tokenizers, needs_devices):\n    print(model_str)\n    print()\n    print()\n    with open(f'/kaggle/working/output_noContext_{model_str}.jsonl', 'w') as jsonl_file:\n        for i in range(1, 20):\n            # Hindu, Muslim, Christian, Sikh, Buddhist, Jain, Parsi\n            if i == 2:\n                continue\n            for c in [None]:\n                for ctype in [None]:\n                    with open(f'/kaggle/input/pewdata/{i}.json') as f:\n                        q = json.load(f)\n\n                    prompts, expected_answers = generate_prompts(q, c, ctype)\n\n                    for idx, prompt in enumerate(prompts):\n                        if len(prompts) != 1:\n                            qno = f\"{i_q_lookup[i]}_{'abcdefghijklmnopqrstuvwxyz'[idx]}\"\n                        else:\n                            qno = f\"{i_q_lookup[i]}\"\n                        dist = get_distribution(prompt, expected_answers, model, tokenizer, n_device)\n\n                        # Create a unique ID (e.g., using `i`, `c`, `ctype`, and `idx` for uniqueness)\n                        unique_id = f\"{qno}_{c}_{ctype}\"\n                        print(unique_id)\n\n                        # Create the JSON object\n                        json_obj = {\n                            'id': unique_id,\n                            'question_number': i,\n                            'context': c,\n                            'context_type': ctype,\n                            'prompt': prompt,\n                            'distribution': dist\n                        }\n\n                        # Write the JSON object as a line in the JSONL file\n                        jsonl_file.write(json.dumps(json_obj) + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:28:24.394117Z","iopub.execute_input":"2024-10-27T17:28:24.394508Z","iopub.status.idle":"2024-10-27T17:28:42.983448Z","shell.execute_reply.started":"2024-10-27T17:28:24.394463Z","shell.execute_reply":"2024-10-27T17:28:42.982383Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"falcon_11b\n\n\nq2_None_None\nq6_a_None_None\nq6_b_None_None\nq6_c_None_None\nq6_d_None_None\nq6_e_None_None\nq7_None_None\nq9_None_None\nABORT_None_None\nq10_None_None\nq11y_a_None_None\nq11y_b_None_None\nq11y_c_None_None\nq11y_d_None_None\nq11y_e_None_None\nq11y_f_None_None\nq11y_g_None_None\nq11y_h_None_None\nq11y_i_None_None\nq12_None_None\nq17_a_None_None\nq17_b_None_None\nq19_a_None_None\nq19_b_None_None\nqw21_a_None_None\nqw21_b_None_None\nqm21_a_None_None\nqm21_b_None_None\nq22_a_None_None\nq22_b_None_None\nq22_c_None_None\nq23_a_None_None\nq23_b_None_None\nq24_None_None\nq26_None_None\nq35_None_None\nq70_None_None\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\n\n# Open the JSONL file in write mode\nfor model_str, model, tokenizer, n_device in zip(model_strs, models, tokenizers, needs_devices):\n    print(model_str)\n    print()\n    print()\n    with open(f'/kaggle/working/output_religionContext_{model_str}.jsonl', 'w') as jsonl_file:\n        for i in range(1, 20):\n            # Hindu, Muslim, Christian, Sikh, Buddhist, Jain, Parsi\n            if i == 2:\n                continue\n            for c in ['hindu', 'muslim', 'christian', 'sikh', 'buddhist', 'jain', 'parsi']:\n                for ctype in ['QA', 'BIO', 'PORTRAY']:\n                    with open(f'/kaggle/input/pewdata/{i}.json') as f:\n                        q = json.load(f)\n\n                    prompts, expected_answers = generate_prompts(q, c, ctype)\n\n                    for idx, prompt in enumerate(prompts):\n                        if len(prompts) != 1:\n                            qno = f\"{i_q_lookup[i]}_{'abcdefghijklmnopqrstuvwxyz'[idx]}\"\n                        else:\n                            qno = f\"{i_q_lookup[i]}\"\n                        dist = get_distribution(prompt, expected_answers, model, tokenizer, n_device)\n\n                        # Create a unique ID (e.g., using `i`, `c`, `ctype`, and `idx` for uniqueness)\n                        unique_id = f\"{qno}_{c}_{ctype}\"\n                        print(unique_id)\n\n                        # Create the JSON object\n                        json_obj = {\n                            'id': unique_id,\n                            'question_number': i,\n                            'context': c,\n                            'context_type': ctype,\n                            'prompt': prompt,\n                            'distribution': dist\n                        }\n\n                        # Write the JSON object as a line in the JSONL file\n                        jsonl_file.write(json.dumps(json_obj) + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:28:42.985010Z","iopub.execute_input":"2024-10-27T17:28:42.985649Z","iopub.status.idle":"2024-10-27T17:36:41.821470Z","shell.execute_reply.started":"2024-10-27T17:28:42.985584Z","shell.execute_reply":"2024-10-27T17:36:41.820272Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"falcon_11b\n\n\nq2_hindu_QA\nq2_hindu_BIO\nq2_hindu_PORTRAY\nq2_muslim_QA\nq2_muslim_BIO\nq2_muslim_PORTRAY\nq2_christian_QA\nq2_christian_BIO\nq2_christian_PORTRAY\nq2_sikh_QA\nq2_sikh_BIO\nq2_sikh_PORTRAY\nq2_buddhist_QA\nq2_buddhist_BIO\nq2_buddhist_PORTRAY\nq2_jain_QA\nq2_jain_BIO\nq2_jain_PORTRAY\nq2_parsi_QA\nq2_parsi_BIO\nq2_parsi_PORTRAY\nq6_a_hindu_QA\nq6_b_hindu_QA\nq6_c_hindu_QA\nq6_d_hindu_QA\nq6_e_hindu_QA\nq6_a_hindu_BIO\nq6_b_hindu_BIO\nq6_c_hindu_BIO\nq6_d_hindu_BIO\nq6_e_hindu_BIO\nq6_a_hindu_PORTRAY\nq6_b_hindu_PORTRAY\nq6_c_hindu_PORTRAY\nq6_d_hindu_PORTRAY\nq6_e_hindu_PORTRAY\nq6_a_muslim_QA\nq6_b_muslim_QA\nq6_c_muslim_QA\nq6_d_muslim_QA\nq6_e_muslim_QA\nq6_a_muslim_BIO\nq6_b_muslim_BIO\nq6_c_muslim_BIO\nq6_d_muslim_BIO\nq6_e_muslim_BIO\nq6_a_muslim_PORTRAY\nq6_b_muslim_PORTRAY\nq6_c_muslim_PORTRAY\nq6_d_muslim_PORTRAY\nq6_e_muslim_PORTRAY\nq6_a_christian_QA\nq6_b_christian_QA\nq6_c_christian_QA\nq6_d_christian_QA\nq6_e_christian_QA\nq6_a_christian_BIO\nq6_b_christian_BIO\nq6_c_christian_BIO\nq6_d_christian_BIO\nq6_e_christian_BIO\nq6_a_christian_PORTRAY\nq6_b_christian_PORTRAY\nq6_c_christian_PORTRAY\nq6_d_christian_PORTRAY\nq6_e_christian_PORTRAY\nq6_a_sikh_QA\nq6_b_sikh_QA\nq6_c_sikh_QA\nq6_d_sikh_QA\nq6_e_sikh_QA\nq6_a_sikh_BIO\nq6_b_sikh_BIO\nq6_c_sikh_BIO\nq6_d_sikh_BIO\nq6_e_sikh_BIO\nq6_a_sikh_PORTRAY\nq6_b_sikh_PORTRAY\nq6_c_sikh_PORTRAY\nq6_d_sikh_PORTRAY\nq6_e_sikh_PORTRAY\nq6_a_buddhist_QA\nq6_b_buddhist_QA\nq6_c_buddhist_QA\nq6_d_buddhist_QA\nq6_e_buddhist_QA\nq6_a_buddhist_BIO\nq6_b_buddhist_BIO\nq6_c_buddhist_BIO\nq6_d_buddhist_BIO\nq6_e_buddhist_BIO\nq6_a_buddhist_PORTRAY\nq6_b_buddhist_PORTRAY\nq6_c_buddhist_PORTRAY\nq6_d_buddhist_PORTRAY\nq6_e_buddhist_PORTRAY\nq6_a_jain_QA\nq6_b_jain_QA\nq6_c_jain_QA\nq6_d_jain_QA\nq6_e_jain_QA\nq6_a_jain_BIO\nq6_b_jain_BIO\nq6_c_jain_BIO\nq6_d_jain_BIO\nq6_e_jain_BIO\nq6_a_jain_PORTRAY\nq6_b_jain_PORTRAY\nq6_c_jain_PORTRAY\nq6_d_jain_PORTRAY\nq6_e_jain_PORTRAY\nq6_a_parsi_QA\nq6_b_parsi_QA\nq6_c_parsi_QA\nq6_d_parsi_QA\nq6_e_parsi_QA\nq6_a_parsi_BIO\nq6_b_parsi_BIO\nq6_c_parsi_BIO\nq6_d_parsi_BIO\nq6_e_parsi_BIO\nq6_a_parsi_PORTRAY\nq6_b_parsi_PORTRAY\nq6_c_parsi_PORTRAY\nq6_d_parsi_PORTRAY\nq6_e_parsi_PORTRAY\nq7_hindu_QA\nq7_hindu_BIO\nq7_hindu_PORTRAY\nq7_muslim_QA\nq7_muslim_BIO\nq7_muslim_PORTRAY\nq7_christian_QA\nq7_christian_BIO\nq7_christian_PORTRAY\nq7_sikh_QA\nq7_sikh_BIO\nq7_sikh_PORTRAY\nq7_buddhist_QA\nq7_buddhist_BIO\nq7_buddhist_PORTRAY\nq7_jain_QA\nq7_jain_BIO\nq7_jain_PORTRAY\nq7_parsi_QA\nq7_parsi_BIO\nq7_parsi_PORTRAY\nq9_hindu_QA\nq9_hindu_BIO\nq9_hindu_PORTRAY\nq9_muslim_QA\nq9_muslim_BIO\nq9_muslim_PORTRAY\nq9_christian_QA\nq9_christian_BIO\nq9_christian_PORTRAY\nq9_sikh_QA\nq9_sikh_BIO\nq9_sikh_PORTRAY\nq9_buddhist_QA\nq9_buddhist_BIO\nq9_buddhist_PORTRAY\nq9_jain_QA\nq9_jain_BIO\nq9_jain_PORTRAY\nq9_parsi_QA\nq9_parsi_BIO\nq9_parsi_PORTRAY\nABORT_hindu_QA\nABORT_hindu_BIO\nABORT_hindu_PORTRAY\nABORT_muslim_QA\nABORT_muslim_BIO\nABORT_muslim_PORTRAY\nABORT_christian_QA\nABORT_christian_BIO\nABORT_christian_PORTRAY\nABORT_sikh_QA\nABORT_sikh_BIO\nABORT_sikh_PORTRAY\nABORT_buddhist_QA\nABORT_buddhist_BIO\nABORT_buddhist_PORTRAY\nABORT_jain_QA\nABORT_jain_BIO\nABORT_jain_PORTRAY\nABORT_parsi_QA\nABORT_parsi_BIO\nABORT_parsi_PORTRAY\nq10_hindu_QA\nq10_hindu_BIO\nq10_hindu_PORTRAY\nq10_muslim_QA\nq10_muslim_BIO\nq10_muslim_PORTRAY\nq10_christian_QA\nq10_christian_BIO\nq10_christian_PORTRAY\nq10_sikh_QA\nq10_sikh_BIO\nq10_sikh_PORTRAY\nq10_buddhist_QA\nq10_buddhist_BIO\nq10_buddhist_PORTRAY\nq10_jain_QA\nq10_jain_BIO\nq10_jain_PORTRAY\nq10_parsi_QA\nq10_parsi_BIO\nq10_parsi_PORTRAY\nq11y_a_hindu_QA\nq11y_b_hindu_QA\nq11y_c_hindu_QA\nq11y_d_hindu_QA\nq11y_e_hindu_QA\nq11y_f_hindu_QA\nq11y_g_hindu_QA\nq11y_h_hindu_QA\nq11y_i_hindu_QA\nq11y_a_hindu_BIO\nq11y_b_hindu_BIO\nq11y_c_hindu_BIO\nq11y_d_hindu_BIO\nq11y_e_hindu_BIO\nq11y_f_hindu_BIO\nq11y_g_hindu_BIO\nq11y_h_hindu_BIO\nq11y_i_hindu_BIO\nq11y_a_hindu_PORTRAY\nq11y_b_hindu_PORTRAY\nq11y_c_hindu_PORTRAY\nq11y_d_hindu_PORTRAY\nq11y_e_hindu_PORTRAY\nq11y_f_hindu_PORTRAY\nq11y_g_hindu_PORTRAY\nq11y_h_hindu_PORTRAY\nq11y_i_hindu_PORTRAY\nq11y_a_muslim_QA\nq11y_b_muslim_QA\nq11y_c_muslim_QA\nq11y_d_muslim_QA\nq11y_e_muslim_QA\nq11y_f_muslim_QA\nq11y_g_muslim_QA\nq11y_h_muslim_QA\nq11y_i_muslim_QA\nq11y_a_muslim_BIO\nq11y_b_muslim_BIO\nq11y_c_muslim_BIO\nq11y_d_muslim_BIO\nq11y_e_muslim_BIO\nq11y_f_muslim_BIO\nq11y_g_muslim_BIO\nq11y_h_muslim_BIO\nq11y_i_muslim_BIO\nq11y_a_muslim_PORTRAY\nq11y_b_muslim_PORTRAY\nq11y_c_muslim_PORTRAY\nq11y_d_muslim_PORTRAY\nq11y_e_muslim_PORTRAY\nq11y_f_muslim_PORTRAY\nq11y_g_muslim_PORTRAY\nq11y_h_muslim_PORTRAY\nq11y_i_muslim_PORTRAY\nq11y_a_christian_QA\nq11y_b_christian_QA\nq11y_c_christian_QA\nq11y_d_christian_QA\nq11y_e_christian_QA\nq11y_f_christian_QA\nq11y_g_christian_QA\nq11y_h_christian_QA\nq11y_i_christian_QA\nq11y_a_christian_BIO\nq11y_b_christian_BIO\nq11y_c_christian_BIO\nq11y_d_christian_BIO\nq11y_e_christian_BIO\nq11y_f_christian_BIO\nq11y_g_christian_BIO\nq11y_h_christian_BIO\nq11y_i_christian_BIO\nq11y_a_christian_PORTRAY\nq11y_b_christian_PORTRAY\nq11y_c_christian_PORTRAY\nq11y_d_christian_PORTRAY\nq11y_e_christian_PORTRAY\nq11y_f_christian_PORTRAY\nq11y_g_christian_PORTRAY\nq11y_h_christian_PORTRAY\nq11y_i_christian_PORTRAY\nq11y_a_sikh_QA\nq11y_b_sikh_QA\nq11y_c_sikh_QA\nq11y_d_sikh_QA\nq11y_e_sikh_QA\nq11y_f_sikh_QA\nq11y_g_sikh_QA\nq11y_h_sikh_QA\nq11y_i_sikh_QA\nq11y_a_sikh_BIO\nq11y_b_sikh_BIO\nq11y_c_sikh_BIO\nq11y_d_sikh_BIO\nq11y_e_sikh_BIO\nq11y_f_sikh_BIO\nq11y_g_sikh_BIO\nq11y_h_sikh_BIO\nq11y_i_sikh_BIO\nq11y_a_sikh_PORTRAY\nq11y_b_sikh_PORTRAY\nq11y_c_sikh_PORTRAY\nq11y_d_sikh_PORTRAY\nq11y_e_sikh_PORTRAY\nq11y_f_sikh_PORTRAY\nq11y_g_sikh_PORTRAY\nq11y_h_sikh_PORTRAY\nq11y_i_sikh_PORTRAY\nq11y_a_buddhist_QA\nq11y_b_buddhist_QA\nq11y_c_buddhist_QA\nq11y_d_buddhist_QA\nq11y_e_buddhist_QA\nq11y_f_buddhist_QA\nq11y_g_buddhist_QA\nq11y_h_buddhist_QA\nq11y_i_buddhist_QA\nq11y_a_buddhist_BIO\nq11y_b_buddhist_BIO\nq11y_c_buddhist_BIO\nq11y_d_buddhist_BIO\nq11y_e_buddhist_BIO\nq11y_f_buddhist_BIO\nq11y_g_buddhist_BIO\nq11y_h_buddhist_BIO\nq11y_i_buddhist_BIO\nq11y_a_buddhist_PORTRAY\nq11y_b_buddhist_PORTRAY\nq11y_c_buddhist_PORTRAY\nq11y_d_buddhist_PORTRAY\nq11y_e_buddhist_PORTRAY\nq11y_f_buddhist_PORTRAY\nq11y_g_buddhist_PORTRAY\nq11y_h_buddhist_PORTRAY\nq11y_i_buddhist_PORTRAY\nq11y_a_jain_QA\nq11y_b_jain_QA\nq11y_c_jain_QA\nq11y_d_jain_QA\nq11y_e_jain_QA\nq11y_f_jain_QA\nq11y_g_jain_QA\nq11y_h_jain_QA\nq11y_i_jain_QA\nq11y_a_jain_BIO\nq11y_b_jain_BIO\nq11y_c_jain_BIO\nq11y_d_jain_BIO\nq11y_e_jain_BIO\nq11y_f_jain_BIO\nq11y_g_jain_BIO\nq11y_h_jain_BIO\nq11y_i_jain_BIO\nq11y_a_jain_PORTRAY\nq11y_b_jain_PORTRAY\nq11y_c_jain_PORTRAY\nq11y_d_jain_PORTRAY\nq11y_e_jain_PORTRAY\nq11y_f_jain_PORTRAY\nq11y_g_jain_PORTRAY\nq11y_h_jain_PORTRAY\nq11y_i_jain_PORTRAY\nq11y_a_parsi_QA\nq11y_b_parsi_QA\nq11y_c_parsi_QA\nq11y_d_parsi_QA\nq11y_e_parsi_QA\nq11y_f_parsi_QA\nq11y_g_parsi_QA\nq11y_h_parsi_QA\nq11y_i_parsi_QA\nq11y_a_parsi_BIO\nq11y_b_parsi_BIO\nq11y_c_parsi_BIO\nq11y_d_parsi_BIO\nq11y_e_parsi_BIO\nq11y_f_parsi_BIO\nq11y_g_parsi_BIO\nq11y_h_parsi_BIO\nq11y_i_parsi_BIO\nq11y_a_parsi_PORTRAY\nq11y_b_parsi_PORTRAY\nq11y_c_parsi_PORTRAY\nq11y_d_parsi_PORTRAY\nq11y_e_parsi_PORTRAY\nq11y_f_parsi_PORTRAY\nq11y_g_parsi_PORTRAY\nq11y_h_parsi_PORTRAY\nq11y_i_parsi_PORTRAY\nq12_hindu_QA\nq12_hindu_BIO\nq12_hindu_PORTRAY\nq12_muslim_QA\nq12_muslim_BIO\nq12_muslim_PORTRAY\nq12_christian_QA\nq12_christian_BIO\nq12_christian_PORTRAY\nq12_sikh_QA\nq12_sikh_BIO\nq12_sikh_PORTRAY\nq12_buddhist_QA\nq12_buddhist_BIO\nq12_buddhist_PORTRAY\nq12_jain_QA\nq12_jain_BIO\nq12_jain_PORTRAY\nq12_parsi_QA\nq12_parsi_BIO\nq12_parsi_PORTRAY\nq17_a_hindu_QA\nq17_b_hindu_QA\nq17_a_hindu_BIO\nq17_b_hindu_BIO\nq17_a_hindu_PORTRAY\nq17_b_hindu_PORTRAY\nq17_a_muslim_QA\nq17_b_muslim_QA\nq17_a_muslim_BIO\nq17_b_muslim_BIO\nq17_a_muslim_PORTRAY\nq17_b_muslim_PORTRAY\nq17_a_christian_QA\nq17_b_christian_QA\nq17_a_christian_BIO\nq17_b_christian_BIO\nq17_a_christian_PORTRAY\nq17_b_christian_PORTRAY\nq17_a_sikh_QA\nq17_b_sikh_QA\nq17_a_sikh_BIO\nq17_b_sikh_BIO\nq17_a_sikh_PORTRAY\nq17_b_sikh_PORTRAY\nq17_a_buddhist_QA\nq17_b_buddhist_QA\nq17_a_buddhist_BIO\nq17_b_buddhist_BIO\nq17_a_buddhist_PORTRAY\nq17_b_buddhist_PORTRAY\nq17_a_jain_QA\nq17_b_jain_QA\nq17_a_jain_BIO\nq17_b_jain_BIO\nq17_a_jain_PORTRAY\nq17_b_jain_PORTRAY\nq17_a_parsi_QA\nq17_b_parsi_QA\nq17_a_parsi_BIO\nq17_b_parsi_BIO\nq17_a_parsi_PORTRAY\nq17_b_parsi_PORTRAY\nq19_a_hindu_QA\nq19_b_hindu_QA\nq19_a_hindu_BIO\nq19_b_hindu_BIO\nq19_a_hindu_PORTRAY\nq19_b_hindu_PORTRAY\nq19_a_muslim_QA\nq19_b_muslim_QA\nq19_a_muslim_BIO\nq19_b_muslim_BIO\nq19_a_muslim_PORTRAY\nq19_b_muslim_PORTRAY\nq19_a_christian_QA\nq19_b_christian_QA\nq19_a_christian_BIO\nq19_b_christian_BIO\nq19_a_christian_PORTRAY\nq19_b_christian_PORTRAY\nq19_a_sikh_QA\nq19_b_sikh_QA\nq19_a_sikh_BIO\nq19_b_sikh_BIO\nq19_a_sikh_PORTRAY\nq19_b_sikh_PORTRAY\nq19_a_buddhist_QA\nq19_b_buddhist_QA\nq19_a_buddhist_BIO\nq19_b_buddhist_BIO\nq19_a_buddhist_PORTRAY\nq19_b_buddhist_PORTRAY\nq19_a_jain_QA\nq19_b_jain_QA\nq19_a_jain_BIO\nq19_b_jain_BIO\nq19_a_jain_PORTRAY\nq19_b_jain_PORTRAY\nq19_a_parsi_QA\nq19_b_parsi_QA\nq19_a_parsi_BIO\nq19_b_parsi_BIO\nq19_a_parsi_PORTRAY\nq19_b_parsi_PORTRAY\nqw21_a_hindu_QA\nqw21_b_hindu_QA\nqw21_a_hindu_BIO\nqw21_b_hindu_BIO\nqw21_a_hindu_PORTRAY\nqw21_b_hindu_PORTRAY\nqw21_a_muslim_QA\nqw21_b_muslim_QA\nqw21_a_muslim_BIO\nqw21_b_muslim_BIO\nqw21_a_muslim_PORTRAY\nqw21_b_muslim_PORTRAY\nqw21_a_christian_QA\nqw21_b_christian_QA\nqw21_a_christian_BIO\nqw21_b_christian_BIO\nqw21_a_christian_PORTRAY\nqw21_b_christian_PORTRAY\nqw21_a_sikh_QA\nqw21_b_sikh_QA\nqw21_a_sikh_BIO\nqw21_b_sikh_BIO\nqw21_a_sikh_PORTRAY\nqw21_b_sikh_PORTRAY\nqw21_a_buddhist_QA\nqw21_b_buddhist_QA\nqw21_a_buddhist_BIO\nqw21_b_buddhist_BIO\nqw21_a_buddhist_PORTRAY\nqw21_b_buddhist_PORTRAY\nqw21_a_jain_QA\nqw21_b_jain_QA\nqw21_a_jain_BIO\nqw21_b_jain_BIO\nqw21_a_jain_PORTRAY\nqw21_b_jain_PORTRAY\nqw21_a_parsi_QA\nqw21_b_parsi_QA\nqw21_a_parsi_BIO\nqw21_b_parsi_BIO\nqw21_a_parsi_PORTRAY\nqw21_b_parsi_PORTRAY\nqm21_a_hindu_QA\nqm21_b_hindu_QA\nqm21_a_hindu_BIO\nqm21_b_hindu_BIO\nqm21_a_hindu_PORTRAY\nqm21_b_hindu_PORTRAY\nqm21_a_muslim_QA\nqm21_b_muslim_QA\nqm21_a_muslim_BIO\nqm21_b_muslim_BIO\nqm21_a_muslim_PORTRAY\nqm21_b_muslim_PORTRAY\nqm21_a_christian_QA\nqm21_b_christian_QA\nqm21_a_christian_BIO\nqm21_b_christian_BIO\nqm21_a_christian_PORTRAY\nqm21_b_christian_PORTRAY\nqm21_a_sikh_QA\nqm21_b_sikh_QA\nqm21_a_sikh_BIO\nqm21_b_sikh_BIO\nqm21_a_sikh_PORTRAY\nqm21_b_sikh_PORTRAY\nqm21_a_buddhist_QA\nqm21_b_buddhist_QA\nqm21_a_buddhist_BIO\nqm21_b_buddhist_BIO\nqm21_a_buddhist_PORTRAY\nqm21_b_buddhist_PORTRAY\nqm21_a_jain_QA\nqm21_b_jain_QA\nqm21_a_jain_BIO\nqm21_b_jain_BIO\nqm21_a_jain_PORTRAY\nqm21_b_jain_PORTRAY\nqm21_a_parsi_QA\nqm21_b_parsi_QA\nqm21_a_parsi_BIO\nqm21_b_parsi_BIO\nqm21_a_parsi_PORTRAY\nqm21_b_parsi_PORTRAY\nq22_a_hindu_QA\nq22_b_hindu_QA\nq22_c_hindu_QA\nq22_a_hindu_BIO\nq22_b_hindu_BIO\nq22_c_hindu_BIO\nq22_a_hindu_PORTRAY\nq22_b_hindu_PORTRAY\nq22_c_hindu_PORTRAY\nq22_a_muslim_QA\nq22_b_muslim_QA\nq22_c_muslim_QA\nq22_a_muslim_BIO\nq22_b_muslim_BIO\nq22_c_muslim_BIO\nq22_a_muslim_PORTRAY\nq22_b_muslim_PORTRAY\nq22_c_muslim_PORTRAY\nq22_a_christian_QA\nq22_b_christian_QA\nq22_c_christian_QA\nq22_a_christian_BIO\nq22_b_christian_BIO\nq22_c_christian_BIO\nq22_a_christian_PORTRAY\nq22_b_christian_PORTRAY\nq22_c_christian_PORTRAY\nq22_a_sikh_QA\nq22_b_sikh_QA\nq22_c_sikh_QA\nq22_a_sikh_BIO\nq22_b_sikh_BIO\nq22_c_sikh_BIO\nq22_a_sikh_PORTRAY\nq22_b_sikh_PORTRAY\nq22_c_sikh_PORTRAY\nq22_a_buddhist_QA\nq22_b_buddhist_QA\nq22_c_buddhist_QA\nq22_a_buddhist_BIO\nq22_b_buddhist_BIO\nq22_c_buddhist_BIO\nq22_a_buddhist_PORTRAY\nq22_b_buddhist_PORTRAY\nq22_c_buddhist_PORTRAY\nq22_a_jain_QA\nq22_b_jain_QA\nq22_c_jain_QA\nq22_a_jain_BIO\nq22_b_jain_BIO\nq22_c_jain_BIO\nq22_a_jain_PORTRAY\nq22_b_jain_PORTRAY\nq22_c_jain_PORTRAY\nq22_a_parsi_QA\nq22_b_parsi_QA\nq22_c_parsi_QA\nq22_a_parsi_BIO\nq22_b_parsi_BIO\nq22_c_parsi_BIO\nq22_a_parsi_PORTRAY\nq22_b_parsi_PORTRAY\nq22_c_parsi_PORTRAY\nq23_a_hindu_QA\nq23_b_hindu_QA\nq23_a_hindu_BIO\nq23_b_hindu_BIO\nq23_a_hindu_PORTRAY\nq23_b_hindu_PORTRAY\nq23_a_muslim_QA\nq23_b_muslim_QA\nq23_a_muslim_BIO\nq23_b_muslim_BIO\nq23_a_muslim_PORTRAY\nq23_b_muslim_PORTRAY\nq23_a_christian_QA\nq23_b_christian_QA\nq23_a_christian_BIO\nq23_b_christian_BIO\nq23_a_christian_PORTRAY\nq23_b_christian_PORTRAY\nq23_a_sikh_QA\nq23_b_sikh_QA\nq23_a_sikh_BIO\nq23_b_sikh_BIO\nq23_a_sikh_PORTRAY\nq23_b_sikh_PORTRAY\nq23_a_buddhist_QA\nq23_b_buddhist_QA\nq23_a_buddhist_BIO\nq23_b_buddhist_BIO\nq23_a_buddhist_PORTRAY\nq23_b_buddhist_PORTRAY\nq23_a_jain_QA\nq23_b_jain_QA\nq23_a_jain_BIO\nq23_b_jain_BIO\nq23_a_jain_PORTRAY\nq23_b_jain_PORTRAY\nq23_a_parsi_QA\nq23_b_parsi_QA\nq23_a_parsi_BIO\nq23_b_parsi_BIO\nq23_a_parsi_PORTRAY\nq23_b_parsi_PORTRAY\nq24_hindu_QA\nq24_hindu_BIO\nq24_hindu_PORTRAY\nq24_muslim_QA\nq24_muslim_BIO\nq24_muslim_PORTRAY\nq24_christian_QA\nq24_christian_BIO\nq24_christian_PORTRAY\nq24_sikh_QA\nq24_sikh_BIO\nq24_sikh_PORTRAY\nq24_buddhist_QA\nq24_buddhist_BIO\nq24_buddhist_PORTRAY\nq24_jain_QA\nq24_jain_BIO\nq24_jain_PORTRAY\nq24_parsi_QA\nq24_parsi_BIO\nq24_parsi_PORTRAY\nq26_hindu_QA\nq26_hindu_BIO\nq26_hindu_PORTRAY\nq26_muslim_QA\nq26_muslim_BIO\nq26_muslim_PORTRAY\nq26_christian_QA\nq26_christian_BIO\nq26_christian_PORTRAY\nq26_sikh_QA\nq26_sikh_BIO\nq26_sikh_PORTRAY\nq26_buddhist_QA\nq26_buddhist_BIO\nq26_buddhist_PORTRAY\nq26_jain_QA\nq26_jain_BIO\nq26_jain_PORTRAY\nq26_parsi_QA\nq26_parsi_BIO\nq26_parsi_PORTRAY\nq35_hindu_QA\nq35_hindu_BIO\nq35_hindu_PORTRAY\nq35_muslim_QA\nq35_muslim_BIO\nq35_muslim_PORTRAY\nq35_christian_QA\nq35_christian_BIO\nq35_christian_PORTRAY\nq35_sikh_QA\nq35_sikh_BIO\nq35_sikh_PORTRAY\nq35_buddhist_QA\nq35_buddhist_BIO\nq35_buddhist_PORTRAY\nq35_jain_QA\nq35_jain_BIO\nq35_jain_PORTRAY\nq35_parsi_QA\nq35_parsi_BIO\nq35_parsi_PORTRAY\nq70_hindu_QA\nq70_hindu_BIO\nq70_hindu_PORTRAY\nq70_muslim_QA\nq70_muslim_BIO\nq70_muslim_PORTRAY\nq70_christian_QA\nq70_christian_BIO\nq70_christian_PORTRAY\nq70_sikh_QA\nq70_sikh_BIO\nq70_sikh_PORTRAY\nq70_buddhist_QA\nq70_buddhist_BIO\nq70_buddhist_PORTRAY\nq70_jain_QA\nq70_jain_BIO\nq70_jain_PORTRAY\nq70_parsi_QA\nq70_parsi_BIO\nq70_parsi_PORTRAY\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip arc.zip *.jsonl","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:36:41.822963Z","iopub.execute_input":"2024-10-27T17:36:41.823507Z","iopub.status.idle":"2024-10-27T17:36:43.010852Z","shell.execute_reply.started":"2024-10-27T17:36:41.823472Z","shell.execute_reply":"2024-10-27T17:36:43.009791Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"updating: output_noContext_gemma.jsonl (deflated 82%)\nupdating: output_noContext_llama_3.jsonl (deflated 80%)\nupdating: output_noContext_llama_3_1.jsonl (deflated 80%)\nupdating: output_noContext_llama_3_2.jsonl (deflated 79%)\nupdating: output_religionContext_gemma.jsonl (deflated 93%)\nupdating: output_religionContext_llama_3.jsonl (deflated 90%)\nupdating: output_religionContext_llama_3_1.jsonl (deflated 90%)\nupdating: output_religionContext_llama_3_2.jsonl (deflated 90%)\nupdating: q3r_output_noContext_gemma.jsonl (deflated 79%)\nupdating: q3r_output_noContext_llama_3.jsonl (deflated 77%)\nupdating: q3r_output_noContext_llama_3_1.jsonl (deflated 78%)\nupdating: q3r_output_noContext_llama_3_2.jsonl (deflated 76%)\nupdating: q3r_output_religionContext_gemma.jsonl (deflated 91%)\nupdating: q3r_output_religionContext_llama_3.jsonl (deflated 89%)\nupdating: q3r_output_religionContext_llama_3_1.jsonl (deflated 89%)\nupdating: q3r_output_religionContext_llama_3_2.jsonl (deflated 89%)\nupdating: output_noContext_gemma_2.jsonl (deflated 81%)\nupdating: output_noContext_mistral_v01.jsonl (deflated 80%)\nupdating: output_religionContext_gemma_2.jsonl (deflated 92%)\nupdating: output_religionContext_mistral_v01.jsonl (deflated 92%)\nupdating: q3r_output_noContext_gemma_2.jsonl (deflated 78%)\nupdating: q3r_output_noContext_mistral_v01.jsonl (deflated 77%)\nupdating: q3r_output_religionContext_gemma_2.jsonl (deflated 90%)\nupdating: q3r_output_religionContext_mistral_v01.jsonl (deflated 89%)\nupdating: output_noContext_falcon_7b.jsonl (deflated 83%)\nupdating: output_noContext_mistral_v03.jsonl (deflated 80%)\nupdating: output_religionContext_falcon_7b.jsonl (deflated 94%)\nupdating: output_religionContext_mistral_v03.jsonl (deflated 92%)\nupdating: q3r_output_noContext_falcon_7b.jsonl (deflated 78%)\nupdating: q3r_output_noContext_mistral_v03.jsonl (deflated 75%)\nupdating: q3r_output_religionContext_falcon_7b.jsonl (deflated 92%)\nupdating: q3r_output_religionContext_mistral_v03.jsonl (deflated 90%)\n  adding: output_noContext_falcon_11b.jsonl (deflated 83%)\n  adding: output_religionContext_falcon_11b.jsonl (deflated 94%)\n  adding: q3r_output_noContext_falcon_11b.jsonl (deflated 78%)\n  adding: q3r_output_religionContext_falcon_11b.jsonl (deflated 92%)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}